<!DOCTYPE html>
<html lang="en">
<head>
  <link rel="shortcut icon" type="image/x-icon" href="/resources/favicon.ico">
  <meta name="description" content="Team Carls iQuHACK Project">
  <meta name="keywords" content="QuantumComputing,QAOA">
  <title>iQuHACK Team Carls'</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="project_page.css">
  <!-- <link rel="stylesheet" href="./assets/main.css"> -->
</head>
<body>

<div class="jumbotron">
  <div class="container text-left">
    <h1 style="font-size: 80px;margin-bottom:0;">
      QSearch
    </h1>
    <h1 style="font-size: 40px;margin-bottom:0;">
      Quantum-powered Optimization
    </h1>
    <br>
    <p>
      Arjun, Ethan, Marcus, Micah, Onkar
    </p>
    <br>
  </div>

  <div class="container text-left">
    <!-- <div class="btn-group btn-group-outline-lg" role="group" aria-label="Top menu"> -->
      <a class="btn btn-primary btn-lg" href="https://github.com/micah-roberson/2023_Quantinuum">Code</a>
      <a class="btn btn-primary btn-lg" href="https://medium.com/mit-6-s089-intro-to-quantum-computing/qsearch-quantum-powered-search-and-rescue-819df3cf16b6">Writeup</a>
      <a class="btn btn-primary btn-lg" href="https://www.iquise.mit.edu/iQuHACK/2023-01-27">iQuHACK Page</a>
    <!-- </div> -->
  </div>
</div>

<div class="container bg-3" id="Demonstration">
  <!-- <h2 class="text-center">Results</h2> -->
  <hr>
  <div class="row vspace-top">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe class="embed-responsive-item" src="https://youtube.com/embed/hnWSNWe_Fnw" allowfullscreen></iframe>
    </div>

    <!-- <div class="col justify-content-center text-center">
       <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
          <source src="assets/lndf_vid.mp4" type="video/mp4">
      </video>
    </div> -->
  </div>
</div>

<div class="container bg-3">
  <h2 class="text-center">Abstract</h2>
  <div class="row">
    <h2 class="text-center"></h2>
  </div>
  <p>
    A robot operating in a household environment will
    see a wide range of different objects. In this paper, we present a
    method to generalize object manipulation skills, acquired from
    a limited number of demonstrations, to novel objects of new
    categories of shapes. Our approach, Local Neural Descriptor
    Fields (L-NDF), utilizes neural descriptors defined on the local
    geometry of the object to effectively transfer manipulation
    demonstrations to novel objects at test time, leveraging the
    shared local geometry of novel objects. We illustrate the efficacy
    of our approach in manipulating novel objects in novel poses
    in both simulation as well as in the real world.
  </p>
  <br>
</div>


<div class="container bg-3" id="Related">
  <h2 class="text-center">Related Projects</h2>
  <hr>

  <div class="row vspace-top">
    <div class="col-sm-3">
        <img src="assets/relational_ndf.gif" class="img-fluid" alt="comet" style="width:100%">
    </div>
    <div class="col-sm-9">
      <div class="paper-title">
          <a href="https://anthonysimeonov.github.io/r-ndf/">Relational Neural Descriptor Fields</a>
      </div>
      <p>
        We present a method for performing tasks involving spatial relations
        between novel object instances initialized in arbitrary poses directly from point
        cloud observations.
        We overcome the key technical challenge of determining task-relevant local coordinate
        frames from a few demonstrations by developing an optimization method based on
        Neural Descriptor Fields (NDFs) and a single annotated 3D keypoint. An energy
        based learning scheme to model the joint configuration of the objects that satisfies
        a desired relational task further improves performance. The method is tested on
        three multi-object rearrangement tasks in simulation and on a real robot.
      </p>
    </div>
  </div>

  <div class="row vspace-top">
    <div class="col-sm-3">
        <img src="assets/ndf_change.gif" class="img-fluid" alt="comet" style="width:100%">
    </div>
    <div class="col-sm-9">
      <div class="paper-title">
          <a href="https://yilundu.github.io/ndf_change/">Robust Change Detection Based on Neural Descriptor Fields</a>
      </div>
      <p>
        We develop an object-level online
        change detection approach that is robust to partially overlapping
        observations and noisy localization results.
        Utilizing the shape completion capability and SE(3)-equivariance
        of Neural Descriptor Fields (NDFs),
        we represent objects with compact shape codes encoding full
        object shapes from partial observations.
        By associating objects via shape code similarity and comparing
        local object-neighbor spatial layout, our proposed approach
        demonstrates robustness to low observation overlap and localization
        noises. We conduct experiments on both synthetic and
        real-world sequences and achieve improved change detection
        results compared to multiple baseline methods.
      </p>
    </div>
  </div>

  <div class="row vspace-top">
    <div class="col-sm-3">
        <img src="assets/mug_cut.gif" class="img-fluid" alt="comet" style="width:100%">
    </div>

    <div class="col-sm-9">
      <div class="paper-title">
          <a href="https://yilundu.github.io/ndf/">Neural Descriptor Fields</a>
      </div>
      <p>
        Neural Descriptor Fields (NDFs) condition on object 3D point clouds, and map continuous 3D coordinates to spatial descriptors.
        NDFs have the key properties of encoding category-level correspondence across shapes and being equivariant to rigid 3D transformations.
        They can represent both points and oriented local coordinate frames in the vicinity of the point cloud, and allow recovering corresponding points/frames across shapes via
        nearest-neighbor search in descriptor space (performed via continuous energy optimization).
        We show NDFs facilitate effecient few-shot learning from demonstration for pick-and-place manipulation tasks.
      </p>
    </div>
  </div>
</div>


</div>
</div>

<div class="container bg-3 text-center" id="Thanks">
  <br>
  <br>
  <br>
  <h2>
    Thanks for Reading!!!
  </h2>
  <br>
  <br>
</div>


<!-- <div class="container bg-3">
  <div class="row">
    <h2 class="text-center">Acknowledgement</h2>
    <hr>
    <p>This work is supported by the NSF Institute for AI and Fundamental Interactions, DARPA Machine Common Sense, NSF grant 2214177, AFOSR grant FA9550-22-1-0249, ONR grant N00014-22-1-2740, MIT-IBM Watson Lab,  MIT Quest for Intelligence and Sony. Anthony Simeonov and Yilun Du are supported in part by NSF Graduate Research Fellowships. We thank the members of the Improbable AI Lab and the Learning and Intelligent Systems Lab for the helpful discussions and feedback on the paper. This webpage template was recycled from <a href="https://yenchenlin.me/nerf-supervision/">here</a>. </p>
  </div>
</div><br><br> -->

</body>
</html>


<!--
TODO:
- Put paper on archive
- Get image for relational ndf
- Get description for relational ndf
- Make colab


-->