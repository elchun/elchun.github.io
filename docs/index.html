<!DOCTYPE html>

<!-- Fancy seeing you here... -->

<html lang="en">
  <head>
    <title>Ethan Chun</title>

    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta name="description" content="Ethan Chun is an undergraduate at MIT" />
    <meta
      name="keywords"
      content="Ethan Chun,elchun,MIT,Artifical Intelligence,Robotics,Machine Learning"
    />

    <link rel="stylesheet" href="stylesheet.css" />
    <link rel="stylesheet" href="professional.css" id="aux_style" />
    <!-- <link rel="stylesheet" href="" id="aux_style"> -->

    <link
      rel="shortcut icon"
      type="image/x-icon"
      href="/resources/favicon.ico"
    />
  </head>
  <body scroll="no">
    <div class="content" id="content">
      <div class="header">
        <!-- <img src="resources/cookie.png" id='profile_pic'/> -->
        <!-- <img src="resources/loaf.png" id='profile_pic'/> -->
        <!-- <img src="resources/ethan_profile_1.jpg" id='profile_pic'/> -->
        <img src="" id="profile_pic" />
        <!-- <h1>Ethan's Super Fun Project Site<h1> -->
        <!-- <h1 id="name">Ethan's Super Fun Project Site</h1> -->
        <h1 id="name">Ethan Chun</h1>
        <!-- <h1 id="name">Ethan Chun</h1> -->
      </div>

      <!-- <hr> -->

      <div>
        <p>
          Hello! Welcome to my project page! I am a Research Engineer at Meta working
          on machine learning for humanoid robots.  Prior to this, I was a master's student
          at MIT with Sang-bae Kim's Biomimetic Robotics Group, working
          on robotic manipulation involving 3D perception and tactile sensing.
          During this time, I also interned with the Optimus team at Tesla where I worked on
          learning-based navigation.
          I have previously worked with Leslie Kaelbling and
          Tomas Lozano-Perez's Learning and Intelligent Systems Group and Hugh
          Herr's Biomechatronics Group.

          <!-- Hello! This is my super fun project site for lots of random projects.
                Honestly, I just thought it'd be kinda fun to slap a bunch of stuff
                together so we'll see how this goes. -->
          <!-- Hello! Welcome to my project page!  I am a junior at MIT studying Comptuer Science
                and Math.  I currently work with the Hugh Herr's Biomechatronics Group and
                Leslie Kaelbling and Tomas Lozano-Perez's Learning and Intelligent Systems Group. -->
        </p>
        <p>
          <b>This section is rather out of date...</b>
          I am particularly interested in the intersection of Robotics and
          Machine Learning with a focus on 3D perception systems for robotic
          manipulation.* I believe that developing robust manipulation and
          perception systems is key to enabling robots to prosper in the real
          world. Some recent projects include my work on Local Neural Descriptor
          Fields, presented at ICRA 2023, and my work on 6-Dof Approach Planning
          with Reflexive Grasp Execution, submitted to ICRA 2024. On my free
          time, I enjoy making random things, dumpster diving, biking, and
          exploring my school's campus.

          <!--
                with a focus on applying machine learning techniques to improve current robotic performance.
                Some of my recent projects include creating a chess playing robot in Drake simulator
                using Mask R-CNN and ICP and developing Local Neural Descriptor Fields
                for locally condition pose extraction. -->
        </p>
        <p>
          My resume can also be found
          <a
            href="resources/Chun_Ethan_Resume.pdf"
            target="_blank"
            style="color: green"
            ><b>here</b></bhere></a
          >
          in case anyone is curious. You can contact me at elchun [at] mit [dot]
          edu. Otherwise, enjoy the projects!
          <!-- I've attached my <a href="resources/Chun_Ethan_Resume.pdf" target="_blank" style="color: green">resume</a> in case
                anyone is curious.  Otherwise, enjoy the projects! -->
        </p>
        <p>
          <i
            >*Recent experience has taught me that perhaps there is a deeper
            issue than perception. If we want robotic agents to exist in the
            real world, then it seems that we should be designing models to
            operate continuously and integrate information over time. I am
            therefore investigating a change of focus toward neuroscience
            inspired AI with a focus on memory systems, adaptation, and
            recurrent architectures.</i
          >
          <!--
          Therefore, broadly speaking, I am interested in investigating the
          challenges that separate our current robotic systems from ones that
          are natively designed for a continuous time world. So far, I believe
          that these challenges relate to the problems of building memory
          systems, designing online adaptation or learning algorithms, building
          expressive and mutable world models, and understanding the role of
          recurrent processing in all these systems. -->
          <!-- <i>Note: I wil be remaining at MIT for a 5th year to complete my Master's Degree.</i> -->
        </p>
        <!-- <i>Note: I wil be remaining at MIT for a 5th year to complete my Master's Degree.</i> -->
      </div>

      <h1>Publications</h1>

      <div class="project_tab">
        <!-- <video src="resources/chessbot_web_clip.gif" class="project_video" autoplay muted loop> -->
        <!-- <source src="resources/chessbot_web_clip.mp4" type="video/mp4"/> -->
        <!-- </video> -->

        <img src="resources/icra_2023_web_smol.gif" class="project_picture" />
        <!-- <iframe width="20%" height="auto" src="https://www.youtube.com/embed/B49J5w8MMrM?autoplay=1&mute=1"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media;
                gyroscope; picture-in-picture" allowfullscreen></iframe>
            <img src="project_pages/maker_portfolio_images/Supermax 4.jpg" class="project_picture"/> -->
        <div>
          <h2>
            <!-- <a href="https://github.com/elchun/ChessBot" target="_blank">ChessBot</a> -->
            <a href="lndf/"
              >Local Neural Descriptor Fields: Locally Conditioned Object
              Representations for Manipulation</a
            >
            <!-- ChessBot -->
          </h2>
          <p>
            <b>Ethan Chun</b>, Yilun Du, Anthony Simeonov, Tomas Lozano-Perez,
            Leslie Kaelbling
          </p>
          <p>
            Neural Descriptor Fields are a way to implicitly represent three
            dimensional geometry in a neural network. Similarly shaped objects
            produce similar descriptor fields, thus one can use Neural
            Descriptor Fields to encode object poses. Unfortunately,
            conventional Neural Descriptor Fields encode information at the
            object level -- a handle on a mug is treated differently than if
            that handle were glued to a bottle. In this paper, we present an
            architecture which restrict Neural Descriptors to local geometry,
            allowing for more varied robotic manipulation.
          </p>
        </div>
      </div>

      <div class="project_tab">
        <!-- <video src="resources/chessbot_web_clip.gif" class="project_video" autoplay muted loop> -->
        <!-- <source src="resources/chessbot_web_clip.mp4" type="video/mp4"/> -->
        <!-- </video> -->

        <img src="resources/science_paper_header.png" class="project_picture" />
        <!-- <iframe width="20%" height="auto" src="https://www.youtube.com/embed/B49J5w8MMrM?autoplay=1&mute=1"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media;
                gyroscope; picture-in-picture" allowfullscreen></iframe>
            <img src="project_pages/maker_portfolio_images/Supermax 4.jpg" class="project_picture"/> -->
        <div>
          <h2>
            <!-- <a href="https://github.com/elchun/ChessBot" target="_blank">ChessBot</a> -->
            <a href="https://www.science.org/doi/10.1126/science.adv3223"
              >Tissue-integrated bionic knee restores versatile legged movement after amputation
            </a>
            <!-- ChessBot -->
          </h2>
          <p>
            Tony Shu, Daniel Levine, Seong Ho Yeon, <b>Ethan Chun</b>, Christopher C. Shallal, John McCullough, Rickard Br√•nemark, Matthew J. Carty, Marco Ferrone, Sean Boerhout, Alexander Ko, Corey L. Sullivan, Gloria Zhu, Michael Nawrot, Matthew Carney, Ged Wieschhoff, Gabriel Friedman, and Hugh Herr
          </p>
          <p>
            This was a much more extensive followup to the below paper which we ultimately published in Science.
            I was mostly involved in the hardware and data collection since this worked used an upgraded
            version of the one mentioned below.

            <br />
            <br />

            Paper: <a href="https://www.science.org/doi/10.1126/science.adv3223">https://www.science.org/doi/10.1126/science.adv3223</a>
            <br />
            MIT News article: <a href="https://news.mit.edu/2025/bionic-knee-integrated-into-tissue-can-restore-natural-movement-0710">https://news.mit.edu/2025/bionic-knee-integrated-into-tissue-can-restore-natural-movement-0710</a>
          </p>
        </div>
      </div>

      <div class="project_tab">
        <!-- <video src="resources/chessbot_web_clip.gif" class="project_video" autoplay muted loop> -->
        <!-- <source src="resources/chessbot_web_clip.mp4" type="video/mp4"/> -->
        <!-- </video> -->

        <img src="resources/biorob_walking.gif" class="project_picture" />
        <!-- <iframe width="20%" height="auto" src="https://www.youtube.com/embed/B49J5w8MMrM?autoplay=1&mute=1"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media;
                gyroscope; picture-in-picture" allowfullscreen></iframe>
            <img src="project_pages/maker_portfolio_images/Supermax 4.jpg" class="project_picture"/> -->
        <div>
          <h2>
            <!-- <a href="https://github.com/elchun/ChessBot" target="_blank">ChessBot</a> -->
            <a href="project_pages/biorob2022.html"
              >Modulation of Prosthetic Ankle Plantarflexion Through Direct
              Myoelectric Control of a Subject-Optimized Neuromuscular Model
            </a>
            <!-- ChessBot -->
          </h2>
          <p>
            Tony Shu, Christopher Shallal, <b>Ethan Chun</b>, Aashini Shah,
            Angel Bu, Daniel Levine, Seong Ho Yeon, Matthew Carney, Hyungeun
            Song, Tsung-Han Hsieh, and Hugh M. Herr
          </p>
          <p>
            The focus of this paper is on a novel EMG control paradigm using a
            Neuromuscular model of trans-tibial amputee subject. However, in
            order to test this paradigm, we also needed a fully function robotic
            ankle -- hence my role. While Tony and Chris focused on the controls
            aspect, I worked on sensor integration, mechanical system upgrades,
            new firmware to accommodate the EMG board, and various other
            mechatronics related tasks. It's a lot of fun building robots!
          </p>
        </div>
      </div>

      <!-- <hr> -->
      <h1>Technical Projects</h1>
      <!-- <hr> -->
      <div class="project_tab">
        <img
          src="equivShapeGen/assets/equivShapeGen.png"
          class="project_picture"
        />

        <div>
          <h2>
            <a href="equivShapeGen"
              >Novel Shape Generation with SO(3)-Equivariant Auto-Encoders</a
            >
          </h2>
          <p>
            This is my final project for Prof. Tess Smidt's course on
            Equivariant Neural Networks. The main idea is that we wanted to find
            a more efficient way to generate 3D shapes. To do so, we leverage
            the inherent SO(3) symmetry in 3D data and propose an SO(3)
            equivariant auto-encoding neural network. We represent 3D shapes as
            the coefficients of the spherical harmonic basis functions and
            design a rotation equivariant interpolation method to traverse the
            latent space. We demonstrate robust reconstruction of a dataset of
            100 randomly generated boxes and show coherent traversals of the
            latent space when trained on two and 100 randomly selected objects.
          </p>
        </div>
      </div>

      <div class="project_tab">
        <img src="resources/probInverseGraphics.png" class="project_picture" />

        <div>
          <h2>
            <a href="probInvGraphics"
              >Understanding Probabilistic Inverse Graphics through Optical
              Digit Recognition with 2D Gaussians</a
            >
          </h2>
          <p>
            This is my final project for Prof. Josh Tenenbaum's course on
            Computational Cognitive Science. I wanted to make a simple
            demonstration of probabilistic inverse graphics. To do so, I
            targeted optical digit recognition, one of the simplest inverse
            graphics problems known. I modeled each digit as a collection of 2D
            Gaussians and ran inference on noisy point set images. I
            demonstrated a success rate of 0.95 over 50 trials per digit.
            Furthermore, I presented two of my past attempts at this problem and
            provided intuitive reasoning on their strengths and pitfalls.
          </p>
        </div>
      </div>

      <div class="project_tab">
        <img src="resources/tinyLDM.gif" class="project_picture" />

        <div>
          <h2>
            <a href="tinyLDM/">Tiny View Conditioned Latent Diffusion Models</a>
          </h2>
          <p>
            View conditioned latent diffusion models show great potential to
            improve scene understanding in robotics applications. However,
            training times are slow and while the former focuses on image
            quality, robotic applications prioritize efficiency. To fix this, we
            present a scaled down version of the popular Latent Diffusion Model,
            trained on the SRNCars dataset. Inspired by the work,
            <a href="https://zero123.cs.columbia.edu/" target="_blank"
              >Zero-1-to-3</a
            >, we provide image and pose conditioning conditioning to allow for
            novel view synthesis from single images. Our model reduces the
            number of trainable parameters from 860 million in state of the art
            models to less than 1.7 million parameters and trains in less than
            four hours on a consumer GPU. This was my final project for
            <a
              href="https://www.scenerepresentations.org/courses/inverse-graphics-23/"
              target="_blank"
            >
              Vincent Sitzmann's Inverse Graphics seminar
            </a>
            at MIT.
          </p>
        </div>
      </div>

      <div class="project_tab">
        <!-- <img src="resources/default_image.jpeg" class="project_picture" > -->
        <img src="resources/stereoOcc.png" class="project_picture" />

        <div>
          <h2>
            <a href="stereoOcc/"> Stereo Occupancy Networks </a>
          </h2>
          <p>
            Current 3D neural implicit reconstruction algorithms often use
            single images or point clouds as input data. However, the noise
            artifacts or lack of information from these data can degrade the
            robustness of their implicit representations. What if we leverage
            recent advances in stereo perception and use stereo images as input
            data? Click here to find out more! This was my final project for
            MIT's Computer Vision course (6.8301).
          </p>
        </div>
      </div>

      <div class="project_tab">
        <img src="resources/catBot.gif" class="project_picture" />
        <div>
          <h2>
            <a href="catBot/"> catBot: Solving the Falling Cat Problem </a>
          </h2>
          <p>
            Have you ever seen a cat fall? They always land on their feet! How
            is this possible? Is it magic? This is the problem I explore in a
            set of papers on the falling cat problem. In the first paper, I
            explore the use of trajectory optimization to solve mimic the cat's
            motion. In the second, I test two Reinforcement Learning Algorithms,
            Soft Actor Critic and Proximal Policy Optimization, and compare
            their performance on solving the problem. Read on to learn how cats
            always land on their feet. Spoilers: it's not magic!
          </p>
        </div>
      </div>

      <div class="project_tab">
        <!-- <video src="resources/chessbot_web_clip.gif" class="project_video" autoplay muted loop> -->
        <!-- <source src="resources/chessbot_web_clip.mp4" type="video/mp4"/> -->
        <!-- </video> -->

        <img src="resources/chessbot_web_clip.gif" class="project_picture" />
        <!-- <iframe width="20%" height="auto" src="https://www.youtube.com/embed/B49J5w8MMrM?autoplay=1&mute=1"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media;
                gyroscope; picture-in-picture" allowfullscreen></iframe>
            <img src="project_pages/maker_portfolio_images/Supermax 4.jpg" class="project_picture"/> -->
        <div>
          <h2>
            <!-- <a href="https://github.com/elchun/ChessBot" target="_blank">ChessBot</a> -->
            <a href="project_pages/chessBot.html">
              ChessBot: A Single View Perception and Manipulation System for
              Robotic Chess
            </a>
            <!-- ChessBot -->
          </h2>
          <p>
            Now presenting, a robot playing chess! Using a single RGBD sensor,
            this robot system can perceive and act on the state of a 3d
            simulated chess board. Winning a 2022 Outstanding Project Award,
            this is my final project for MIT's course on robotic manipulation.
          </p>
          <!-- <p>
                    <a href="project_pages/chessBot.html">Website</a>
                    | <a href="https://youtu.be/B49J5w8MMrM">Video</a>
                    | <a href="https://github.com/elchun/ChessBot" target="_blank">Github</a>
                </p> -->
          <!-- <p>
                    Behold, a robot playing chess!  This is my final project for MIT's
                    <a href="https://manipulation.csail.mit.edu/" target="_blank">course</a> on robotic manipulation (taught by Russ Tedrake).
                    While chess playing robots have been built before, almost all rely on
                    custom chess boards, overhead camera systems, or many, many, cameras.  My idea
                    was to use a single, side-mounted, camera which would be inexpensive
                    and easy to install.  I then used Mask R-CNN and ICP to extract the
                    complete board, Stockfish as the chess engine, and Differential Inverse
                    Kinematics to plan the moves.
                    I also used <a href="https://drake.mit.edu/" target="_blank">Drake</a> simulator
                    to simulate a Panda arm and the board.
                </p> -->
        </div>
      </div>

      <div class="project_tab">
        <!-- <img src="resources/cookie.png" class="project_picture"/> -->
        <img src="resources/tf8_knee.gif" class="project_picture" />
        <div>
          <h2>
            <a href="project_pages/tf8_knee.html"
              >Mechatronics Upgrades to the TF8 Powered Prosthetic Knee</a
            >
          </h2>
          <p>
            Over the past year, I've been working with
            <a
              href="https://www.media.mit.edu/people/tonyshu/overview/"
              target="_blank"
              >Tony Shu</a
            >
            to upgrade and refine the
            <a
              href="https://ieeexplore.ieee.org/abstract/document/9492290"
              target="_blank"
              >TF8 actuator system</a
            >
            for use as a powered prosthetic knee. On my part, this involved
            various custom designed hardware, included a new endstop system and
            electronics mounts.
          </p>
        </div>
      </div>

      <div class="project_tab">
        <img src="resources/verdin_demo.gif" class="project_picture" />
        <div>
          <h2>
            <a href="project_pages/verdin.html"
              >A Single-board Mechatronics Platform for Control of a Powered
              Prosthesis
            </a>
          </h2>
          <p>
            During my early time at Biomechatronics, I worked extensively with
            <a
              href="https://www.media.mit.edu/people/dvlevine/overview/"
              target="_blank"
              >Dan Levine</a
            >
            and
            <a
              href="https://www.media.mit.edu/people/tonyshu/overview/"
              target="_blank"
              >Tony Shu</a
            >
            to develop a new Prosthesis Controller. This system, dubbed Talaria
            Verdin, enables much of the complex movement seen in the
            Biomechatronics publication above and is currently in use on three
            powered prosthesis systems.
          </p>
        </div>
      </div>

      <h1>Random Projects</h1>

      <div class="project_tab">
        <!-- <img src="resources/cookie.png" class="project_picture"/> -->
        <img src="resources/bird.gif" class="project_picture" />
        <div>
          <h2>
            <a href="project_pages/bird.html">Bird Lamp</a>
          </h2>
          <p>
            I made this bird lamp for a friend's birthday. It's designed after
            the small birds hopping around MIT's campus. The shell was resin
            printed and the lights are controlled by a small arduino-like
            microcontroller.
          </p>
        </div>
      </div>

      <div class="project_tab">
        <!-- <img src="resources/cookie.png" class="project_picture"/> -->
        <img
          src="project_pages/maker_portfolio_images/Supermax 4.jpg"
          class="project_picture"
        />
        <div>
          <h2>
            <a href="project_pages/maker_portfolio.html">Maker Portfolio</a>
          </h2>
          <p>
            This is a collection of images and descriptions from the maker
            portfolio that I used to apply to MIT. I haven't edited it too much
            but its a good overview of some of my previous projects
          </p>
        </div>
      </div>

      <h1>Resources</h1>

      These are links to some quick writeups I've done on relevant topics.
      <h2>
        <a href="resources/Bayes_Filtering.pdf">Bayes Filtering Derivation</a>
      </h2>

      <h2>
        <a href="resources/Surface_Normal_Sensor_Writeup.pdf"
          >Computing the surface normal of an object using ToF data</a
        >
      </h2>

      :0
      <hr />

      <!-- <h1>Research</h1>

        <div class="project_tab">
            <img src="project_pages/maker_portfolio_images/Supermax 4.jpg" class="project_picture"/>
            <div>
                <h2>
                    <a href="project_pages/maker_portfolio.html">ChessBot</a>
                </h2>
                <p>
                    ChessBot. TODO
                </p>
            </div>
        </div> -->

      <!-- <a href="project_pages/maker_portfolio.html">Maker Portfolio</a>
        <ul>
            <li>
            <p>
            This is a collection of images and descriptions from the maker portfolio
            that I used to apply to MIT.  I haven't edited it too much but its a good
            overview of some of my previous projects
            </p>
            </li>
        </ul> -->

      <!-- <a href="project_pages/publications.html">Research</a>
        <ul>
            <li>
            <p>
            I really enjoy working on reasearch, see more of it here!
            </p>
            </li>
        </ul> -->
    </div>

    <canvas id="board" width="100%" height="100%"></canvas>

    <button type="button" id="fish_on">Turn fish off!</button>

    <button type="button" id="style_button">Fun mode!</button>

    <style></style>
  </body>

  <script src="fish.js"></script>
  <script src="switch_mode.js"></script>
</html>

<!-- TODO:
Make cursor into icecream
Add reserach

Add fun mode button
Find lcd -->
