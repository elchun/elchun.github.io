<!DOCTYPE html>
<html>
<head>
  <title>Novel View Synthesis from Single Images with Tiny Latent Diffusion Models</title>
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <style>
    /* Custom CSS for additional styles */
    body {
      background-color: #f4f4f4;
    }
    .img-max {
      max-width: 800px;
    }
    .header {
      background-color: #f4f4f4;
    }


  </style>
</head>
<body>
  <div class="container-fluid">
    <!-- <div class="row sticky-top header"> -->
    <div class="row">
      <div class="col">
        <h1 class="display-3 text-black">Novel View Synthesis from Single Images with Tiny Latent Diffusion Models</h1>
        <h2 class="display-6 text-black"> <a href="https://elchun.github.io/" style="text-decoration: none">Ethan Chun</a></h2>
        <p>Available as a colab notebook <a href="https://colab.research.google.com/drive/1leJiaBDRsB_otVcb4GGfE0TlXTuxMJtD" target="_blank">here</a>.</p>
      </div>
    </div>
    <div class="row">
      <div class="col">
        <hr/>
      </div>
    </div>

    <div class="row">
      <div class="col">
        <h1>Abstract</h1>

        <ul class="list-group">
          <li class="list-group-item">
            <b>Context:</b>
            View conditioned diffusion models show great potential to improve scene understanding in robotics applications.
          </li>
          <li class="list-group-item">
            <b>Problem:</b>
            However, training times are slow and the goals of current diffusion models and while the former focuses on image quality,
            robotic applications prioritize efficiency.
          </li>
          <li class="list-group-item">
            <b>Solution:</b>  We present a scaled down version of the popular Latent Diffusion Model, trained on the SRNCars dataset.
            We provide view conditioning to allow for novel view synthesis from single images and demonstrate reasonable
            image outputs on a single object class dataset.
            Our model reduces the number of trainable parameters from 860 million in state of the art models to less than 1.7 million parameters
            and trains in less than four hours on a consumer GPU.
          </li>
          <!-- <li class="list-group-item">
            <b>Results</b> We find that stereo occupancy networks can outperform traditional monocular occupancy networks in
            reconstructing some objects. However, the average volumetric intersection over union scores (IoU) of our stereo
            occupancy network were 40% lower than those of a similar monocular occupancy network.
          </li> -->
        </ul>
      </div>
    </div>

    <div class="row justify-content-center">
      <h1>Summary</h1>
      <div class="col-lg-8">
        <div style="position: relative; padding-bottom: 56.25%; padding-top: 25px; height: 0;">
          <iframe
            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"
            src="https://www.youtube.com/embed/SXjXzLqLGrE"
            title="YouTube video player"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen
          ></iframe>
        </div>
      </div>
    </div>


    <div class="row justify-content-center">
      <h1>Results</h1>
      <p class="lead">
        Given a single input image (left) and a conditioning transform (not shown), our tiny latent
        diffusion model generates novel views (pred) of unseen geometry. Note that even when the model has no information on the top or back of
        the cars, it can still generate feasible predictions that resemble the unseen ground truth images.
      </p>
      <div class="col-lg-8">
        <!-- <img src="assets/recon.png" alt="Image" class="img-fluid"> -->
        <img src="assets/6s980_results.png" alt="Image" class="img-fluid" >
      </div>
    </div>

    <div class="row">
      <div class="col">
        <h1>Paper</h1>
        <p class="lead">
          Please see our paper for additional details on the project.
        </p>
        <div class="row justify-content-center">
          <img src="assets/elchun_6s980_p1.png" alt="Paper" class="img-fluid img-max">
          <img src="assets/elchun_6s980_p2.png" alt="Paper" class="img-fluid img-max">
          <img src="assets/elchun_6s980_p3.png" alt="Paper" class="img-fluid img-max">
          <img src="assets/elchun_6s980_p4.png" alt="Paper" class="img-fluid img-max">
          <img src="assets/elchun_6s980_p5.png" alt="Paper" class="img-fluid img-max">
        </div>
      </div>
    </div>

    <div class="row">
      <p class="lead">
        Webpage created by Ethan Chun and ChatGPT :&#41;
      </p>
    </div>
  </div>

</body>
</html>
